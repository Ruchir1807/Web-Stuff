{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO6n7rvx0JQf93LArT1kNmj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ruchir1807/Web-Stuff/blob/main/moosicgenre.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "SycELPbTBWw1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b042edbd-d66d-4b6f-e07f-800e9e137a53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!pip install pydub\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d andradaolteanu/gtzan-dataset-music-genre-classification\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLDJrcSHB0g1",
        "outputId": "e5cd983d-d502-4d43-db7e-d41ebc1cd64f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification\n",
            "License(s): other\n",
            "gtzan-dataset-music-genre-classification.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/gtzan-dataset-music-genre-classification.zip','r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "WVgOplVKCK_e"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "file_name = '/content/Data/genres_original'\n",
        "file_path = '/content/Data/genres_original/metal/metal.00001.wav'\n",
        "\n",
        "\n",
        "def convert_wav_format(file_path):\n",
        "    \"\"\"Convert wav file to a format readable by librosa\"\"\"\n",
        "    sound = AudioSegment.from_wav(file_path)\n",
        "    sound = sound.set_frame_rate(22050).set_channels(1)\n",
        "    new_file_path = file_path.replace(\".wav\", \"_converted.wav\")\n",
        "    sound.export(new_file_path, format=\"wav\")\n",
        "    return new_file_path\n",
        "\n",
        "\n",
        "\n",
        "def extract_features(file_name):\n",
        "    try:\n",
        "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') #res_type='kaiser_fast' specifies the resampling algorithm, which is a fast and efficient method.\n",
        "        # Extracting MFCCs(Mel-Frequency Cepstral Coefficients) captures timbral aspects of sound\n",
        "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "        mfccsscaled = np.mean(mfccs.T,axis=0)\n",
        "\n",
        "        # Extracting Mel Spectrogram, A Mel Spectrogram represents the intensity of frequencies over time, mapped to the Mel scale, which is closer to human perception of sound.\n",
        "        mel = librosa.feature.melspectrogram(y=audio, sr=sample_rate)\n",
        "        melsscaled = np.mean(mel.T, axis=0)\n",
        "\n",
        "        # Extracting Chroma Feature,Chroma features represent the energy of each pitch class (like notes in music) over time.\n",
        "\n",
        "        chroma = librosa.feature.chroma_stft(y=audio, sr=sample_rate)\n",
        "        chroma_scaled = np.mean(chroma.T, axis=0)\n",
        "\n",
        "        return np.hstack([mfccsscaled, melsscaled, chroma_scaled])\n",
        "    except Exception as e:\n",
        "        print(f\"Error encountered while parsing file: {file_name}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "x8sJLnv4B4qy"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Path to your audio files\n",
        "data_dir = \"/content/Data/genres_original\"\n",
        "\n",
        "# Prepare lists to hold features and labels\n",
        "features_list = []\n",
        "labels_list = []\n",
        "\n",
        "# Loop over all files in your dataset\n",
        "for genre in os.listdir(data_dir):\n",
        "    genre_path = os.path.join(data_dir, genre)\n",
        "    for file in os.listdir(genre_path):\n",
        "        file_path = os.path.join(genre_path, file)\n",
        "        features = extract_features(file_path)\n",
        "        if features is not None:\n",
        "            features_list.append(features)\n",
        "            labels_list.append(genre)\n",
        "\n",
        "# Convert to DataFrame for easier processing\n",
        "features_df = pd.DataFrame(features_list)\n",
        "labels_df = pd.DataFrame(labels_list, columns=['label'])\n",
        "\n",
        "# Combine the features and labels into one DataFrame\n",
        "data = pd.concat([features_df, labels_df], axis=1)\n",
        "\n",
        "# Display the first few rows of the data\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sllDBdHTCkaj",
        "outputId": "05f3f90e-0fbd-4eb7-e4b4-7e4695b169ca"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error encountered while parsing file: /content/Data/genres_original/reggae/smells-like-teen-spirit-_-nirvana-_-no-copyright-_-made-with-Voicemod.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-23e7123e2996>:21: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') #res_type='kaiser_fast' specifies the resampling algorithm, which is a fast and efficient method.\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error encountered while parsing file: /content/Data/genres_original/reggae/.ipynb_checkpoints\n",
            "Error encountered while parsing file: /content/Data/genres_original/jazz/jazz.00054.wav\n",
            "            0           1          2          3          4          5  \\\n",
            "0 -185.063400  120.512108  29.720230  56.372219  -3.028252  22.213934   \n",
            "1  -24.671669   62.440269  -7.754089  28.432596  10.997148   5.551390   \n",
            "2 -302.410675  150.116394  -3.713836  26.951956  -0.261415  13.506062   \n",
            "3 -141.014435  123.921112  -2.510380  27.954958  -0.004282   1.210126   \n",
            "4  -20.817461   64.907516   6.767101  26.357128  11.665423  13.185630   \n",
            "\n",
            "           6          7          8         9  ...       171       172  \\\n",
            "0 -10.350042  20.053120 -13.679162  9.170144  ...  0.394442  0.257475   \n",
            "1  -0.086744  13.745090   2.899817  9.909151  ...  0.366442  0.442675   \n",
            "2   7.233895   4.227106   1.757852  6.571226  ...  0.400966  0.440800   \n",
            "3 -11.015174  -1.365306  -7.019753 -6.129560  ...  0.216462  0.360627   \n",
            "4  -1.979107   8.582198   0.472895  7.441096  ...  0.520297  0.361389   \n",
            "\n",
            "        173       174       175       176       177       178       179  \\\n",
            "0  0.320463  0.274815  0.449038  0.427794  0.290351  0.288845  0.172923   \n",
            "1  0.351655  0.403609  0.522922  0.370621  0.411171  0.332561  0.444168   \n",
            "2  0.313504  0.324178  0.258067  0.357865  0.413941  0.404331  0.470518   \n",
            "3  0.392850  0.318980  0.452540  0.254133  0.172284  0.155700  0.263377   \n",
            "4  0.385516  0.312900  0.427536  0.579396  0.398901  0.430524  0.350754   \n",
            "\n",
            "     label  \n",
            "0  country  \n",
            "1  country  \n",
            "2  country  \n",
            "3  country  \n",
            "4  country  \n",
            "\n",
            "[5 rows x 181 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Encode labels\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(labels_df['label'])\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_df, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Dense(256, input_shape=(X_train.shape[1],), activation='relu')) #input layer\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(128, activation='relu'))#hidden layer\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(10, activation='softmax'))#outer layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "model.save(\"genre_classification_model.keras\") #saving the model for loading later\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the previously saved model\n",
        "model1 = load_model(\"genre_classification_model.keras\")\n",
        "\n",
        "# Define a function to predict the genre of a new audio file\n",
        "def predict_genre(file_path, model):\n",
        "    # Extract features from the audio file\n",
        "    features = extract_features(file_path)\n",
        "    if features is not None:\n",
        "        # Reshape features to match the input format of the classifier\n",
        "        features = features.reshape(1, -1)\n",
        "        # Predict the genre\n",
        "        prediction = model.predict(features)\n",
        "         # Find the index of the genre with the highest probability\n",
        "        predicted_index = np.argmax(prediction)\n",
        "\n",
        "        # List of genres corresponding to the indices\n",
        "        genre_labels = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
        "\n",
        "        # Return the predicted genre\n",
        "        return genre_labels[predicted_index]\n",
        "\n",
        "    else:\n",
        "        print(\"Failed to extract features from the audio file.\")\n",
        "        return None\n",
        "\n",
        "# Path to the new audio file\n",
        "new_audio_file = \"/content/Data/genres_original/metal/metal.00001.wav\"\n",
        "\n",
        "# Predict the genre of the new audio file\n",
        "predicted_genre = predict_genre(new_audio_file, model1)\n",
        "print(f\"Predicted Genre: {predicted_genre}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGZi9L17IX8D",
        "outputId": "c2f39552-740a-475c-c534-6054a5cc9b5a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - accuracy: 0.1128 - loss: 24.7147 - val_accuracy: 0.2900 - val_loss: 5.2237\n",
            "Epoch 2/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2822 - loss: 8.9832 - val_accuracy: 0.3150 - val_loss: 2.9922\n",
            "Epoch 3/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2694 - loss: 4.8015 - val_accuracy: 0.3000 - val_loss: 2.2433\n",
            "Epoch 4/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2667 - loss: 2.9998 - val_accuracy: 0.3350 - val_loss: 2.0867\n",
            "Epoch 5/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2605 - loss: 2.7424 - val_accuracy: 0.2750 - val_loss: 2.0990\n",
            "Epoch 6/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2815 - loss: 2.6118 - val_accuracy: 0.3150 - val_loss: 1.9658\n",
            "Epoch 7/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2932 - loss: 2.2587 - val_accuracy: 0.3100 - val_loss: 1.9469\n",
            "Epoch 8/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3283 - loss: 2.1189 - val_accuracy: 0.3450 - val_loss: 1.9210\n",
            "Epoch 9/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3537 - loss: 2.0055 - val_accuracy: 0.3400 - val_loss: 1.8718\n",
            "Epoch 10/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3933 - loss: 1.9559 - val_accuracy: 0.4050 - val_loss: 1.7809\n",
            "Epoch 11/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3090 - loss: 2.1175 - val_accuracy: 0.4600 - val_loss: 1.7887\n",
            "Epoch 12/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4336 - loss: 1.8670 - val_accuracy: 0.4500 - val_loss: 1.7594\n",
            "Epoch 13/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3991 - loss: 1.8354 - val_accuracy: 0.4200 - val_loss: 1.7306\n",
            "Epoch 14/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3984 - loss: 1.7787 - val_accuracy: 0.4550 - val_loss: 1.7164\n",
            "Epoch 15/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4370 - loss: 1.7278 - val_accuracy: 0.4500 - val_loss: 1.7392\n",
            "Epoch 16/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4734 - loss: 1.6411 - val_accuracy: 0.5050 - val_loss: 1.6410\n",
            "Epoch 17/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4210 - loss: 1.7135 - val_accuracy: 0.4650 - val_loss: 1.6057\n",
            "Epoch 18/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4788 - loss: 1.5933 - val_accuracy: 0.5250 - val_loss: 1.5926\n",
            "Epoch 19/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4629 - loss: 1.7537 - val_accuracy: 0.4800 - val_loss: 1.6256\n",
            "Epoch 20/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4221 - loss: 1.5842 - val_accuracy: 0.5250 - val_loss: 1.4986\n",
            "Epoch 21/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5110 - loss: 1.4763 - val_accuracy: 0.5250 - val_loss: 1.4883\n",
            "Epoch 22/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5093 - loss: 1.4225 - val_accuracy: 0.5650 - val_loss: 1.4508\n",
            "Epoch 23/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4982 - loss: 1.4386 - val_accuracy: 0.5650 - val_loss: 1.4820\n",
            "Epoch 24/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5008 - loss: 1.5127 - val_accuracy: 0.5800 - val_loss: 1.4135\n",
            "Epoch 25/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5524 - loss: 1.3385 - val_accuracy: 0.5600 - val_loss: 1.4086\n",
            "Epoch 26/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5072 - loss: 1.3908 - val_accuracy: 0.5800 - val_loss: 1.3436\n",
            "Epoch 27/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5586 - loss: 1.2582 - val_accuracy: 0.5600 - val_loss: 1.3666\n",
            "Epoch 28/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5331 - loss: 1.2609 - val_accuracy: 0.5700 - val_loss: 1.3565\n",
            "Epoch 29/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5541 - loss: 1.3026 - val_accuracy: 0.5650 - val_loss: 1.3376\n",
            "Epoch 30/30\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5677 - loss: 1.1517 - val_accuracy: 0.5700 - val_loss: 1.3740\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6013 - loss: 1.3679 \n",
            "Test Accuracy: 0.5699999928474426\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
            "Predicted Genre: metal\n"
          ]
        }
      ]
    }
  ]
}